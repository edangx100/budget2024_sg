import os
import logging
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI as OpenAILamaindex
from llama_index.postprocessor.cohere_rerank import CohereRerank
from llama_index.core.indices.query.query_transform import HyDEQueryTransform
from llama_index.core.query_engine import TransformQueryEngine
from llama_index.agent.openai import OpenAIAgent
from llama_index.core.tools import QueryEngineTool, FunctionTool, ToolMetadata
from llama_index.retrievers.bm25 import BM25Retriever
from openai import OpenAI
import chromadb
import streamlit as st
from prompts import GUARD_PROMPT, CITATION_PROMPT_A, CITATION_PROMPT_B, CLASSIFY_QUERY_PROMPT, AGENT_PROMPT, QUERY_ENGINE_BUDGET_DESCRIPTION, QUERY_ENGINE_HOUSEHOLD_DESCRIPTION


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("./logs/app.log", mode='a'),
    ]
)


def llm_chat_completion(client: OpenAI, query: str) -> str:
    """
    Generates a chat completion response from the OpenAI model.

    Args:
        client (OpenAI): The OpenAI client instance used to make the API call.
        query (str): The user query for which a response is to be generated.

    Returns:
        str: The content of the response generated by the model.
    """
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": "You are a helpful assistant."}, 
                  {"role": "user", "content": query}],
        temperature=0.0
    )
    return completion.choices[0].message.content

def evaluate_query(client: OpenAI, retrieved_nodes: str, query: str) -> str:
    """
    Evaluates a query using a classification prompt.

    Args:
        client (OpenAI): The OpenAI client instance used to make the API call.
        retrieved_nodes (str): The text of the nodes retrieved by the retriever.
        query (str): The user query to be evaluated.

    Returns:
        str: The content of the response generated by the model.
    """
    query = CLASSIFY_QUERY_PROMPT.format(query_str=query, retrieved_nodes=retrieved_nodes)
    return llm_chat_completion(client, query)

# used for function calling
def evaluate_and_select_vectorstore(keyword_retriever_budget: BM25Retriever, keyword_retriever_household: BM25Retriever, query: str) -> str:
    """Useful to determine which vector store(s) to use."""
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    # Retrieve nodes using the budget retriever, format them into a text list, and evaluate the query against these nodes.
    retrieved_nodes_budget = keyword_retriever_budget.retrieve(query)
    retrieved_nodes_budget_text_list = [f"Node {i+1}\n{node.text}\n" for i, node in enumerate(retrieved_nodes_budget)]
    retrieved_nodes_budget_text = "".join(retrieved_nodes_budget_text_list)
    print("retrieved_nodes_budget_text: ", retrieved_nodes_budget_text)
    result_budget = evaluate_query(client, retrieved_nodes_budget_text , query)
    print("result_budget: ", result_budget)

    # Retrieve nodes using the household retriever, format them into a text list, and evaluate the query against these nodes.
    retrieved_nodes_household = keyword_retriever_household.retrieve(query)
    retrieved_nodes_household_text_list = [f"Node {i+1}\n{node.text}\n" for i, node in enumerate(retrieved_nodes_household)]
    retrieved_nodes_household_text = "".join(retrieved_nodes_household_text_list)
    print("retrieved_nodes_household_text: ", retrieved_nodes_household_text)
    result_household = evaluate_query(client, retrieved_nodes_household_text, query)
    print("result_household: ", result_household)

    # Determine which vector store(s) to use based on the evaluation results
    if result_budget == "YES" and result_household == "YES":
        return "Both"
    elif result_budget == "YES":
        return "Budget"
    elif result_household == "YES":
        return "Household"
    else:
        return "Neither"
    
def create_budget_query_engine(embed_model: OpenAIEmbedding, cohere_rerank: CohereRerank) -> tuple:
    """
    Creates and returns a budget query engine with reranking and HyDE transformation.

    Args:
        embed_model (OpenAIEmbedding): The embedding model used for creating the index.
        cohere_rerank (CohereRerank): The reranking model used to improve query results.

    Returns:
        tuple: A tuple containing the reranked query engine and the HyDE transformed query engine.
    """
    # Load from disk
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    chroma_collection = chroma_client.get_collection("budget_collection")
    vector_store_budget = ChromaVectorStore(chroma_collection=chroma_collection)

    # Create index with embedding model
    index_budget = VectorStoreIndex.from_vector_store(
        vector_store_budget,
        embed_model=embed_model
    )

    # Create reranked query engine
    query_engine_budgetRerank = index_budget.as_query_engine(
        similarity_top_k=15,
        node_postprocessors=[cohere_rerank],
        text_qa_template=CITATION_PROMPT_A,
        response_mode="compact",
        verbose=True,
    )

    if query_engine_budgetRerank:
        logging.info("query_engine_budgetRerank setup successfully")
    else:
        logging.error("Failed to setup query_engine_budgetRerank")

    # Add HyDE transformation
    hyde_budget = HyDEQueryTransform(include_original=True)
    query_engine_budgetRerank_HyDE = TransformQueryEngine(query_engine_budgetRerank, hyde_budget)
    
    return query_engine_budgetRerank, query_engine_budgetRerank_HyDE

def create_household_query_engine(embed_model: OpenAIEmbedding, cohere_rerank: CohereRerank) -> tuple:
    """
    Creates and returns a household query engine with reranking and HyDE transformation.

    Args:
        embed_model (OpenAIEmbedding): The embedding model used for creating the index.
        cohere_rerank (CohereRerank): The reranking model used to improve query results.

    Returns:
        tuple: A tuple containing the reranked query engine and the HyDE transformed query engine.
    """
    # Load from disk
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    chroma_collection = chroma_client.get_collection("budget_household_collection")
    vector_store_household = ChromaVectorStore(chroma_collection=chroma_collection)

    # Create index with embedding model
    index_household = VectorStoreIndex.from_vector_store(
        vector_store_household,
        embed_model=embed_model
    )

    # Create reranked query engine
    query_engine_householdReRank = index_household.as_query_engine(
        similarity_top_k=10,
        node_postprocessors=[cohere_rerank],
        text_qa_template=CITATION_PROMPT_B,
        response_mode="compact",
        verbose=True,
    )
    if query_engine_householdReRank:
        logging.info("query_engine_householdReRank setup successfully")
    else:
        logging.error("Failed to setup query_engine_householdReRank")

    # Add HyDE transformation
    hyde_household = HyDEQueryTransform(include_original=True)
    query_engine_householdReRank_HyDE = TransformQueryEngine(query_engine_householdReRank, hyde_household)
    
    return query_engine_householdReRank, query_engine_householdReRank_HyDE

def load_agent(_query_engine_budget: QueryEngineTool, _query_engine_household: QueryEngineTool, function_llm: OpenAILamaindex, bm25_budget: BM25Retriever, bm25_household: BM25Retriever) -> OpenAIAgent:
    """
    Loads and returns an OpenAIAgent configured with query engines and tools.

    Args:
        _query_engine_budget (QueryEngineTool): The query engine tool for budget-related queries.
        _query_engine_household (QueryEngineTool): The query engine tool for household-related queries.
        llm (OpenAILamaindex): The language model to be used by the agent.
        bm25_budget (BM25Retriever): The BM25 retriever for budget-related queries.
        bm25_household (BM25Retriever): The BM25 retriever for household-related queries.

    Returns:
        OpenAIAgent: An agent configured with the specified query engines and tools.
    """
    query_engine_tools = [

        QueryEngineTool(
            query_engine=_query_engine_budget,
            metadata=ToolMetadata(
                name="vector_tool_budget",
                description=QUERY_ENGINE_BUDGET_DESCRIPTION
            ),
        ),

        QueryEngineTool(
            query_engine=_query_engine_household,
            metadata=ToolMetadata(
                name="vector_tool_household",
                description=QUERY_ENGINE_HOUSEHOLD_DESCRIPTION
            ),
        ),

        FunctionTool.from_defaults(
            fn=lambda query: evaluate_and_select_vectorstore(bm25_budget, bm25_household, query),
            name="evaluate_vectorstore",
            description="A tool to determine which vector store(s) to use based on the query content."
        )
    ]

    agent = OpenAIAgent.from_tools(
        query_engine_tools,
        llm=function_llm,
        verbose=True,
        system_prompt= AGENT_PROMPT.format(guard_prompt=GUARD_PROMPT),
    )
    return agent

@st.cache_resource
def initialize_engines() -> tuple:
    """
    Sets up the embedding model, language model, and reranking model.
    It creates query engines for budget and household data, and loads BM25 retrievers
    for both categories.

    Returns:
        tuple: A tuple containing the budget and household HyDE-transformed query engines,
               the language model, and the BM25 retrievers for budget and household data.
    """
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")
    llm = OpenAILamaindex("gpt-4o-mini", temperature=0.2)
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    # Initialize reranking model
    cohere_rerank = CohereRerank(top_n=5, model="rerank-english-v3.0")
    
    # Create query engines
    query_engine_budgetRerank, query_engine_budgetRerank_HyDE = create_budget_query_engine(embed_model, cohere_rerank)
    query_engine_householdReRank, query_engine_householdReRank_HyDE = create_household_query_engine(embed_model, cohere_rerank)
    
    # Load BM25 retrievers
    bm25_retriever_budget = BM25Retriever.from_persist_dir("./budget2024_keyword")
    if bm25_retriever_budget:
        logging.info("bm25_retriever_budget setup successfully")
    else:
        logging.error("Failed to setup bm25_retriever_budget")  
    bm25_retriever_household = BM25Retriever.from_persist_dir("./budget2024_household_keyword")
    if bm25_retriever_household:
        logging.info("bm25_retriever_household setup successfully")
    else:
        logging.error("Failed to setup bm25_retriever_household")
    
    return (query_engine_budgetRerank_HyDE, query_engine_householdReRank_HyDE, 
            bm25_retriever_budget, bm25_retriever_household)


def main():
    load_dotenv()
    
    # Get cached engines
    (query_engine_budgetRerank_HyDE, query_engine_householdReRank_HyDE,
     bm25_retriever_budget, bm25_retriever_household) = initialize_engines()
    
    llm_agent = OpenAILamaindex("gpt-4o-mini", temperature=0.7)

    # Load the agent with the cached engines
    agent = load_agent(
        query_engine_budgetRerank_HyDE,
        query_engine_householdReRank_HyDE,
        llm_agent,
        bm25_retriever_budget,
        bm25_retriever_household
    )

    # Streamlit UI
    st.title("Singapore Budget 2024 Q&A")

    # Initialize session state for chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input
    if prompt := st.chat_input("Ask about Singapore Budget 2024"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            for response in agent.stream_chat(prompt).response_gen:
                full_response += response
                message_placeholder.markdown(full_response + "▌")
            message_placeholder.markdown(full_response)

        st.session_state.messages.append({"role": "assistant", "content": full_response})


    # # ------------------------------------ #
    # # response_stream = agent.stream_chat("What is my personal income tax in 2024?")
    # # response_stream = agent.stream_chat("What is personal income tax operating revenue in 2024?")
    # # response_stream = agent.stream_chat("What is Singapore's operating revenue for personal income tax in 2024?")

    # # response_stream = agent.stream_chat("What are the Key POINTS in Budget 2024?")


    # response_stream = agent.stream_chat("What are the Key reasons for high inflation over the last two years?")
    # # response_stream = agent.stream_chat("What are the reasons for higher cost of living over the last two years?")
    # # response_stream = agent.stream_chat("How is Singapore's economy doing?")


    # # response_stream = agent.stream_chat("What are the payouts i can expect to receive in December 2024?")
    # # response_stream = agent.stream_chat("What can i get in Dec 2024?")



    # # response_stream = agent.stream_chat("Am I elligble for MAJULAH package?")
    # # response_stream = agent.stream_chat("Can i get MAJULAH package?")
    # # response_stream = agent.stream_chat("Do i have MAJULAH package?")
    # # response_stream = agent.stream_chat("What is elligblilty for MAJULAH package?")

    # # response_stream = agent.stream_chat("What is Assurance package?")
    # # response_stream = agent.stream_chat("When can i get Assurance package?")
    # # response_stream = agent.stream_chat("What can i get for Assurance package?")

    # # response_stream = agent.stream_chat("What can i get for Medisave?")
    # # response_stream = agent.stream_chat("Can i get for Medisave?")


    # # response_stream = agent.stream_chat("What is Comlink package? Can i get Comlink package?")      # !
    # # response_stream = agent.stream_chat("What is Comlink package?")
    # # response_stream = agent.stream_chat("Can i get Comlink package?")

    # # response_stream = agent.stream_chat("Who is Lawrence Wong?")



    # # response_stream = agent.stream_chat("What is Budget 2023 key points?")
    # # response_stream = agent.stream_chat("What is AI Singapore?")

    # response_stream.print_response_stream()

    # # Print source nodes if they exist
    # if hasattr(response_stream, 'source_nodes'):
    #     print("\nSource Nodes:")
    #     for idx, node in enumerate(response_stream.source_nodes):
    #         print(f"Node ID: {node.node_id}")

if __name__ == "__main__":
    main()